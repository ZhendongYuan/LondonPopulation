---
title: "moa_enriched_XGboost"
author: "************ yuan"
date: "2019/9/22"
output: html_document
---



```{r setup, results='hide', message=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("dplyr")
library("h2o")
library("ggplot2")
library("rsample") 
library("Metrics")
library("BBmisc")
library("rgdal")
```

# load pop
```{r} 
# at here the scaled_moa_feature.Rdata stands for only scaled populaiton 
moa_data = get(load("/home/z/************/london/enriched_feature/moa_enriched_covariates.Rdata"))
summary(moa_data)
```
# sperate training and testing set
```{r}
#h2o.shutdown(prompt = TRUE)
h2o.no_progress()
h2o.init(max_mem_size = "20g")
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
moa_data.h2o = as.h2o(moa_data)
split <- h2o.splitFrame(moa_data.h2o,c(0.7),seed=123)
train.h2o <- h2o.assign(split[[1]], "train.hex")   
test.h2o <- h2o.assign(split[[2]], "test.hex")  
y <- "population"
x <- setdiff(names(train.h2o), y)
```

# XGboost in moa level (deeper one )
# default setting base line
```{r}
# training basic GBM model with defaults
h2o.fit1 <- h2o.xgboost(
  training_frame = train.h2o,        ## the H2O frame for training
  x = x,
  y = y,
                       ntrees = 50,
                       max_depth = 8,
                       min_rows = 1,
                       learn_rate = 0.1,
                       sample_rate = 0.7,
                       col_sample_rate = 0.9,
                       nfolds = 5,
                       fold_assignment = "Modulo",
                       keep_cross_validation_predictions = TRUE
)

# assess model results
h2o.fit1
```

# randomDiscrete search

```{r eval = FALSE}
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]


# hyperparameter grid
hyper_grid.h2o <- list(
  max_depth = seq(5,50, by = 5),
  learn_rate = c(0.1,0.05,0.08),
  sample_rate = c(.70,0.75, .80),
  col_sample_rate = c(0.75, .80,0.85,0.9),
  reg_alpha = c(1e-4, 1e-2, 0.1, 1, 100)
  
  
)
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mae",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*60
  )

# build grid search 
random_grid <- h2o.grid(
  algorithm = "xgboost",
  grid_id = "xgboost_grid2",
  x = x, 
  y = y, 
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria,
  ntrees = 3000,
  stopping_rounds = 10,
  stopping_tolerance = 0
  )

# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
  grid_id = "xgboost_grid2", 
  sort_by = "mae", 
  decreasing = FALSE
  )
print(grid_perf2)

```


# best model
```{r eval = FALSE}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
best_model@parameters$ntrees
h2o.performance(model = best_model, valid = TRUE)
```



# best model 

```{r}
# train final model
h2o.final <- h2o.xgboost(
  x = x,
  y = y,
  training_frame = train.h2o,
  
  
  ntrees = 3000,
max_depth = 40,
learn_rate = 0.05,
sample_rate = 0.8,
col_sample_rate = 0.8,
reg_alpha = 0.01,
nfolds = 5,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,

  seed = 123
  
)

# model stopped after xx trees
h2o.final@parameters$ntrees

# cross validated RMSE
h2o.rmse(h2o.final, xval = TRUE)

h2o.mae(h2o.final, xval = TRUE)
```

# varaibles important
```{r}
h2o.varimp_plot(h2o.final, num_of_features = 10)
```

# prediction in moa's test dataset.
```{r}
# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)

test_moa_predict = as.data.frame(h2o.predict(h2o.final, test.h2o))

predict_pop = test_moa_predict$predict
real_pop =  as.data.frame(test.h2o)$population
# rmse
rmse(real_pop,predict_pop)

# %rmse
(rmse(real_pop,predict_pop)*length(real_pop))/sum(real_pop)

#mae
mae(real_pop,predict_pop)

# %mae
(mae(real_pop,predict_pop) * length(real_pop))/sum(real_pop)

```

# test directly apply on loa
```{r}
# read loa feature table
#/home/z/************/london/enriched_feature/moa_enriched_covariates.Rdata
loa_data= get(load("/home/z/************/london/enriched_feature/loa_enriched_covariates.Rdata"))
# convert test set to h2o object
loa_transferred.h2o <- as.h2o(loa_data)

# evaluate performance on new data
weighted_loa_performance = h2o.performance(model = h2o.final, newdata = loa_transferred.h2o)

weighted_loa_performance
(weighted_loa_performance@metrics$RMSE*length(loa_data$population))/sum(loa_data$population)
(weighted_loa_performance@metrics$mae*length(loa_data$population))/sum(loa_data$population)


loa_transfer_predict = as.data.frame(h2o.predict(h2o.final, loa_transferred.h2o))



```

# exam the same number of test dataset
```{r}
test = sample_n(loa_data,length(moa_data$population))
# convert test set to h2o object
testloa_samesize.h2o <- as.h2o(test)

# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = testloa_samesize.h2o)
#loa_predict = as.data.frame(h2o.predict(h2o.final, testloa_samesize.h2o))
```
# check response distribution 
```{r}
plot(density(loa_data$population),col="black")
lines(density(moa_data$population),col="green")
lines(density(loa_transfer_predict$predict),col="red")
lines(density(test_moa_predict$predict),col = "blue")
lines(density(as.data.frame(test.h2o)$population),col = "purple")
```

# denormalization
```{r}
moa_or_data= get(load("/home/z/************/london/Rspace/pop/raw_pop_mid_london.Rdata"))
loa_or_data= get(load("/home/z/************/london/Rspace/pop/raw_pop_london_low.Rdata"))
sum_or_pop = sum(moa_or_data$population)

sum_Scaled_pop = sum(loa_transfer_predict$predict)
rough_ratio_red = sum_or_pop/sum_Scaled_pop
descaled_direct_predict_pop = loa_transfer_predict$predict*rough_ratio_red
# ratio_directly
rough_ratio_red
# estimate rmse
rmse(loa_or_data$population,descaled_direct_predict_pop)
(rmse(loa_or_data$population,descaled_direct_predict_pop)*length(loa_or_data$population))/sum(loa_or_data$population)
mae(loa_or_data$population,descaled_direct_predict_pop)
(mae(loa_or_data$population,descaled_direct_predict_pop) * length(loa_or_data$population))/sum(loa_or_data$population)


```

# plot
```{r}
plot(density(loa_or_data$population),col="black")
lines(density(moa_or_data$population),col="green")
lines(density(descaled_direct_predict_pop),col="red")
#lines(density(descaled_test_moa_predict),col = "blue")
#lines(density(as.data.frame(test.h2o)$population),col = "purple")
```


# check the residual correlation
```{r}
#target_data$population,loa_transferred_predict$predict

de_nor_result_df = as.data.frame(cbind(descaled_direct_predict_pop,loa_or_data$population))
colnames(de_nor_result_df) = c("descaled_direct_predict_pop","loa_pop")
de_result_df.lm = lm(loa_pop ~ descaled_direct_predict_pop, data=de_nor_result_df) 
plot(loa_or_data$population,descaled_direct_predict_pop,xlim = c(0,10000),ylim = c(0,10000))
abline(0,1, col='blue')
abline(de_result_df.lm,col="red")
summary(de_result_df.lm)
```

# check the correlation with B_area_sum
```{r}
residuls_tf = abs(de_nor_result_df$descaled_direct_predict_pop-de_nor_result_df$loa_pop)

print("spearman correlation")
cor_matrix_2 <- cor(de_nor_result_df, use = 'complete.obs')
cor_matrix_2

plot(density(residuls_tf))


plot(loa_data$B_AREA_SUM,residuls_tf)

#residul_df = as.data.frame(cbind(loa_or_data,de_nor_result_df$descaled_loa_transferred_predict,residuls_tf))

```


# check the spatial dsitribution is correct
```{r}
loa_shape <- readOGR("/home/z/************/london/enriched_feature/loa_enriched.shp")

loa_shape@data["D_pred_xg"] = as.integer(descaled_direct_predict_pop)
loa_shape@data["T_pop_xg"] = loa_or_data$population
loa_shape@data["residual"] = loa_shape@data$D_pred_xg - loa_shape@data$T_pop_xg
loa_shape@data["residual_abs"] = abs(loa_shape@data$residual)
writeOGR(loa_shape,dsn ="/home/z/************/london/enriched_feature/XGboost",layer = "2_moa2loa_xgboost_spatial",driver = "ESRI Shapefile")
```

